{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trim DataFiles and split into Train, Eval, Test - All Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is too much data to train and test efficiently using the google colab free resources, below is a just Oscar data processing version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "paths = [str(x) for x in Path(\"./Data/\").glob(\"esperanto/*/*sentences.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_prop = .02\n",
    "test_prop = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data\\\\esperanto\\\\oscardata\\\\eo_dedup_sentences.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_edit = set([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\johnc\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ae759dd4b145a8b7015cd3ab563890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING Data\\esperanto\\epo_literature_2011_300K\\epo_literature_2011_300K-sentences.txt\n",
      "1\t0.10 La trajno forlasis Vincovci (malfrue). 0.30 La trajno eniris neĝblokon. 0.37 La sonorilo de Ratchett sonas.\n",
      "\n",
      "0.10 La trajno forlasis Vincovci (malfrue). 0.30 La trajno eniris neĝblokon. 0.37 La sonorilo de Ratchett sonas.\n",
      "\n",
      "PROCESSING Data\\esperanto\\epo_mixed_2012_1M\\epo_mixed_2012_1M-sentences.txt\n",
      "1\tLa retejo de la Asocio (sube) enhavas multan informon pri la Asocio, kaj pri ĝia agado.\n",
      "\n",
      "La retejo de la Asocio (sube) enhavas multan informon pri la Asocio, kaj pri ĝia agado.\n",
      "\n",
      "PROCESSING Data\\esperanto\\epo_newscrawl_2017_1M\\epo_newscrawl_2017_1M-sentences.txt\n",
      "1\t0000: Interrompo AgnoskasTio estas speciala formo de erudicia ciklo implicite traktita al la interromporegilo, kiu resendas interrompovektoron.\n",
      "\n",
      "0000: Interrompo AgnoskasTio estas speciala formo de erudicia ciklo implicite traktita al la interromporegilo, kiu resendas interrompovektoron.\n",
      "\n",
      "PROCESSING Data\\esperanto\\epo_web_2012_1M\\epo_web_2012_1M-sentences.txt\n",
      "1\tLa kultura flanko de la kongreso ne estas forgesata.\n",
      "\n",
      "La kultura flanko de la kongreso ne estas forgesata.\n",
      "\n",
      "PROCESSING Data\\esperanto\\epo_wikipedia_2016_300K\\epo_wikipedia_2016_300K-sentences.txt\n",
      "1\t\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING Data\\esperanto\\OscarData\\eo_dedup_sentences.txt\n",
      "Ĉu ... preĝi | mediti | ricevi instigojn || kanti | muziki || informiĝi | legi | studi || prepari Diservon\n",
      "\n",
      "\n",
      "all files have 4338810 examples out of 4300438 lines\n"
     ]
    }
   ],
   "source": [
    "txt_data = []\n",
    "cnt = 0\n",
    "for path in tqdm_notebook(paths):\n",
    "    print('PROCESSING {}'.format(path))\n",
    "    with open(path, 'r', encoding = 'utf-8') as f:\n",
    "        for ind,l in enumerate(f):\n",
    "            if ind == 0:\n",
    "                print(l)\n",
    "                if path in to_edit:\n",
    "                    print(re.sub('[0-9]*\\t','', l))\n",
    "            if path in to_edit:\n",
    "                txt = re.sub('[0-9]*\\t','', l)\n",
    "            else:\n",
    "                txt = l\n",
    "            cnt += 1\n",
    "            if len(txt) > 1000:\n",
    "                txts = re.split(r\"(?<!^)\\s*[.\\n]+\\s*(?!$)\", txt)\n",
    "                current_line = ''\n",
    "                lines = []\n",
    "                for txt in txts:\n",
    "                    if len(current_line) > 500:\n",
    "                        lines.append(current_line)\n",
    "                        current_line = ''\n",
    "                    current_line += txt\n",
    "                    current_line += '.'\n",
    "                if len(current_line) < 50:\n",
    "                    lines[-1] += current_line\n",
    "                else:\n",
    "                    lines.append(current_line + '.')\n",
    "                txt_data.extend(lines)\n",
    "            else:\n",
    "                txt_data.append(txt)\n",
    "print('{} have {} examples out of {} lines'.format('all files', len(txt_data), cnt))\n",
    "train, valid = train_test_split(txt_data, test_size = valid_prop, random_state = 42)\n",
    "train, test = train_test_split(train, test_size = test_prop, random_state = 42)\n",
    "prefix, suffix= path.rsplit('.',1)\n",
    "TrainFile = open('./Data/esperanto/train.txt', 'w', encoding = 'utf-8')\n",
    "TrainFile.writelines(train)\n",
    "TrainFile.close()\n",
    "ValidFile = open('./Data/esperanto/valid.txt', 'w', encoding = 'utf-8')\n",
    "ValidFile.writelines(valid)\n",
    "ValidFile.close()\n",
    "TestFile = open('./Data/esperanto/test.txt', 'w', encoding = 'utf-8')\n",
    "TestFile.writelines(test)\n",
    "TestFile.close()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and split Oscar Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used instead of above section because of training time issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [str(x) for x in Path(\"./Data/\").glob(\"esperanto/OscarData/*sentences.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_prop = .02\n",
    "test_prop = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, 'r', encoding = 'utf-8') as f:\n",
    "    txt_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700438"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(txt_data, test_size = valid_prop, random_state = 42)\n",
    "train, test = train_test_split(train, test_size = test_prop, random_state = 42)\n",
    "prefix, suffix= path.rsplit('.',1)\n",
    "TrainFile = open('./Data/esperanto/train.txt', 'w', encoding = 'utf-8')\n",
    "TrainFile.writelines(train)\n",
    "TrainFile.close()\n",
    "ValidFile = open('./Data/esperanto/valid.txt', 'w', encoding = 'utf-8')\n",
    "ValidFile.writelines(valid)\n",
    "ValidFile.close()\n",
    "TestFile = open('./Data/esperanto/test.txt', 'w', encoding = 'utf-8')\n",
    "TestFile.writelines(test)\n",
    "TestFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data\\\\esperanto\\\\test.txt', 'Data\\\\esperanto\\\\train.txt', 'Data\\\\esperanto\\\\valid.txt']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path(\"./Data/\").glob(\"esperanto/*.txt\")]\n",
    "print(paths)\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./Preprocessing/BERT-Esperanto\\\\vocab.json',\n",
       " './Preprocessing/BERT-Esperanto\\\\merges.txt']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(\"./Preprocessing/BERT-Esperanto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 5002\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "good_tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./Preprocessing/BERT-Esperanto/vocab.json\",\n",
    "    \"./Preprocessing/BERT-Esperanto/merges.txt\",\n",
    ")\n",
    "good_tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", good_tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", good_tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "\n",
    "good_tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./Preprocessing/BERT-Esperanto/\", max_len=512)\n",
    "\n",
    "txt = \"Mi estas Julien.\" * 1000\n",
    "\n",
    "print(\n",
    "    len(good_tokenizer.encode(txt).tokens),\n",
    "    len(bad_tokenizer.encode(txt))\n",
    ")\n",
    "\n",
    "# results: 512, 5002\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is checking the lengths of encodings for the data, this is a sanity check to make sure all/most don't go over the maximum positional embedding space(or else you'll get a nasty error thrown) - additionally the tokenizer performance with trimming the sequence lengths does not seem to be consist throughout the huggingface libraries, some tokenizers limit to maximum sequence length and some do not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\johnc\\anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558ffe577e414fcab577b5bd59b9b5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=679564.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook\n",
    "lengths = defaultdict(int)\n",
    "with open('./Data/esperanto/train.txt', 'r', encoding = 'utf-8') as f:\n",
    "    for l in tqdm_notebook(f.readlines()):\n",
    "        lengths[tokenizer.encode(l).__len__()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24097,\n",
       " 13489,\n",
       " 9056,\n",
       " 5972,\n",
       " 5680,\n",
       " 5650,\n",
       " 5526,\n",
       " 4846,\n",
       " 4577,\n",
       " 4359,\n",
       " 4051,\n",
       " 3928,\n",
       " 3666,\n",
       " 3396,\n",
       " 3237,\n",
       " 3081,\n",
       " 3078,\n",
       " 3032,\n",
       " 3017,\n",
       " 3013]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lengths.keys(), reverse = True)[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
