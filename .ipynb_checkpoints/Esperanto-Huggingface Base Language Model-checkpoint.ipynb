{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EsperantoDataset(Dataset):\n",
    "    def __init__(self, evaluate: bool = False):\n",
    "        tokenizer = ByteLevelBPETokenizer(\n",
    "            \"./Preprocessing/BERT-Esperanto/vocab.json\",\n",
    "        \"./Preprocessing/BERT-Esperanto/merges.txt\"\n",
    "        )\n",
    "        tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        )\n",
    "        tokenizer.enable_truncation(max_length=512)\n",
    "        # or use the RobertaTokenizer from `transformers` directly.\n",
    "\n",
    "        self.examples = []\n",
    "\n",
    "        src_files = Path(\"./Data/esperanto/\").glob(\"*/*-valid.txt\") if evaluate else Path(\"./Data/esperanto/\").glob(\"*/*-train.txt\")\n",
    "        for src_file in src_files:\n",
    "            print(\"ðŸ”¥\", src_file)\n",
    "            lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "            self.examples += [x.ids for x in tokenizer.encode_batch(lines)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Weâ€™ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=50_000,\n",
    "    max_position_embeddings=128, #512\n",
    "    num_attention_heads=1, #12\n",
    "    num_hidden_layers=1, #6\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./Preprocessing/BERT-Esperanto/\", max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46821200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "#dataset = EsperantoDataset()\n",
    "\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "train_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./Data/esperanto/valid.txt\",\n",
    "    block_size=1028,\n",
    ")\n",
    "\n",
    "valid_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./Data/esperanto/test.txt\",\n",
    "    block_size=1028,\n",
    ")\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Models/Esperanto-Huggingface/\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=20,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=valid_dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a5d1f099624c9a85260f0ac25862e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=10.0, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5a92e96e3247f0b92bd80b375bdc0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=5381.0, style=ProgressStyle(description_wâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.11 GiB (GPU 0; 8.00 GiB total capacity; 5.13 GiB already allocated; 1.22 GiB free; 5.16 GiB reserved in total by PyTorch) (malloc at ..\\c10\\cuda\\CUDACachingAllocator.cpp:289)\n(no backtrace available)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\johnc\\repos\\transformers\\src\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m    461\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m                 \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_training_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m                 if (step + 1) % self.args.gradient_accumulation_steps == 0 or (\n",
      "\u001b[1;32mc:\\users\\johnc\\repos\\transformers\\src\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_training_step\u001b[1;34m(self, model, inputs, optimizer)\u001b[0m\n\u001b[0;32m    590\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\johnc\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\johnc\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.11 GiB (GPU 0; 8.00 GiB total capacity; 5.13 GiB already allocated; 1.22 GiB free; 5.16 GiB reserved in total by PyTorch) (malloc at ..\\c10\\cuda\\CUDACachingAllocator.cpp:289)\n(no backtrace available)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = RobertaConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaModel(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_num_labels\": 2,\n",
       "  \"architectures\": null,\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"do_sample\": false,\n",
       "  \"early_stopping\": false,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"is_decoder\": false,\n",
       "  \"is_encoder_decoder\": false,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"length_penalty\": 1.0,\n",
       "  \"max_length\": 20,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"min_length\": 0,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"no_repeat_ngram_size\": 0,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_beams\": 1,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_return_sequences\": 1,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"pruned_heads\": {},\n",
       "  \"repetition_penalty\": 1.0,\n",
       "  \"temperature\": 1.0,\n",
       "  \"top_k\": 50,\n",
       "  \"top_p\": 1.0,\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_bfloat16\": false,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class PreTrainedModel(nn.Module, ModuleUtilsMixin):\n",
      "    r\"\"\" Base class for all models.\n",
      "\n",
      "        :class:`~transformers.PreTrainedModel` takes care of storing the configuration of the models and handles methods for loading/downloading/saving models\n",
      "        as well as a few methods common to all models to (i) resize the input embeddings and (ii) prune heads in the self-attention heads.\n",
      "\n",
      "        Class attributes (overridden by derived classes):\n",
      "            - ``config_class``: a class derived from :class:`~transformers.PretrainedConfig` to use as configuration class for this model architecture.\n",
      "            - ``pretrained_model_archive_map``: a python ``dict`` of with `short-cut-names` (string) as keys and `url` (string) of associated pretrained weights as values.\n",
      "            - ``load_tf_weights``: a python ``method`` for loading a TensorFlow checkpoint in a PyTorch model, taking as arguments:\n",
      "\n",
      "                - ``model``: an instance of the relevant subclass of :class:`~transformers.PreTrainedModel`,\n",
      "                - ``config``: an instance of the relevant subclass of :class:`~transformers.PretrainedConfig`,\n",
      "                - ``path``: a path (string) to the TensorFlow checkpoint.\n",
      "\n",
      "            - ``base_model_prefix``: a string indicating the attribute associated to the base model in derived classes of the same architecture adding modules on top of the base model.\n",
      "    \"\"\"\n",
      "    config_class = None\n",
      "    pretrained_model_archive_map = {}\n",
      "    base_model_prefix = \"\"\n",
      "\n",
      "    @property\n",
      "    def dummy_inputs(self):\n",
      "        \"\"\" Dummy inputs to do a forward pass in the network.\n",
      "\n",
      "        Returns:\n",
      "            torch.Tensor with dummy inputs\n",
      "        \"\"\"\n",
      "        return {\"input_ids\": torch.tensor(DUMMY_INPUTS)}\n",
      "\n",
      "    def __init__(self, config, *inputs, **kwargs):\n",
      "        super().__init__()\n",
      "        if not isinstance(config, PretrainedConfig):\n",
      "            raise ValueError(\n",
      "                \"Parameter config in `{}(config)` should be an instance of class `PretrainedConfig`. \"\n",
      "                \"To create a model from a pretrained model use \"\n",
      "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
      "                    self.__class__.__name__, self.__class__.__name__\n",
      "                )\n",
      "            )\n",
      "        # Save config in model\n",
      "        self.config = config\n",
      "\n",
      "    @property\n",
      "    def base_model(self):\n",
      "        return getattr(self, self.base_model_prefix, self)\n",
      "\n",
      "    def get_input_embeddings(self):\n",
      "        \"\"\"\n",
      "        Returns the model's input embeddings.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`nn.Module`:\n",
      "                A torch module mapping vocabulary to hidden states.\n",
      "        \"\"\"\n",
      "        base_model = getattr(self, self.base_model_prefix, self)\n",
      "        if base_model is not self:\n",
      "            return base_model.get_input_embeddings()\n",
      "        else:\n",
      "            raise NotImplementedError\n",
      "\n",
      "    def set_input_embeddings(self, value):\n",
      "        \"\"\"\n",
      "        Set model's input embeddings\n",
      "\n",
      "        Args:\n",
      "            value (:obj:`nn.Module`):\n",
      "                A module mapping vocabulary to hidden states.\n",
      "        \"\"\"\n",
      "        base_model = getattr(self, self.base_model_prefix, self)\n",
      "        if base_model is not self:\n",
      "            base_model.set_input_embeddings(value)\n",
      "        else:\n",
      "            raise NotImplementedError\n",
      "\n",
      "    def get_output_embeddings(self):\n",
      "        \"\"\"\n",
      "        Returns the model's output embeddings.\n",
      "\n",
      "        Returns:\n",
      "            :obj:`nn.Module`:\n",
      "                A torch module mapping hidden states to vocabulary.\n",
      "        \"\"\"\n",
      "        return None  # Overwrite for models with output embeddings\n",
      "\n",
      "    def tie_weights(self):\n",
      "        \"\"\"\n",
      "        Tie the weights between the input embeddings and the output embeddings.\n",
      "        If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning\n",
      "        the weights instead.\n",
      "        \"\"\"\n",
      "        output_embeddings = self.get_output_embeddings()\n",
      "        if output_embeddings is not None:\n",
      "            self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())\n",
      "\n",
      "    def _tie_or_clone_weights(self, output_embeddings, input_embeddings):\n",
      "        \"\"\" Tie or clone module weights depending of weither we are using TorchScript or not\n",
      "        \"\"\"\n",
      "        if self.config.torchscript:\n",
      "            output_embeddings.weight = nn.Parameter(input_embeddings.weight.clone())\n",
      "        else:\n",
      "            output_embeddings.weight = input_embeddings.weight\n",
      "\n",
      "        if getattr(output_embeddings, \"bias\", None) is not None:\n",
      "            output_embeddings.bias.data = torch.nn.functional.pad(\n",
      "                output_embeddings.bias.data,\n",
      "                (0, output_embeddings.weight.shape[0] - output_embeddings.bias.shape[0],),\n",
      "                \"constant\",\n",
      "                0,\n",
      "            )\n",
      "        if hasattr(output_embeddings, \"out_features\") and hasattr(input_embeddings, \"num_embeddings\"):\n",
      "            output_embeddings.out_features = input_embeddings.num_embeddings\n",
      "\n",
      "    def resize_token_embeddings(self, new_num_tokens=None):\n",
      "        \"\"\" Resize input token embeddings matrix of the model if new_num_tokens != config.vocab_size.\n",
      "        Take care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.\n",
      "\n",
      "        Arguments:\n",
      "\n",
      "            new_num_tokens: (`optional`) int:\n",
      "                New number of tokens in the embedding matrix. Increasing the size will add newly initialized vectors at the end. Reducing the size will remove vectors from the end.\n",
      "                If not provided or None: does nothing and just returns a pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.\n",
      "\n",
      "        Return: ``torch.nn.Embeddings``\n",
      "            Pointer to the input tokens Embeddings Module of the model\n",
      "        \"\"\"\n",
      "        base_model = getattr(self, self.base_model_prefix, self)  # get the base model if needed\n",
      "        model_embeds = base_model._resize_token_embeddings(new_num_tokens)\n",
      "        if new_num_tokens is None:\n",
      "            return model_embeds\n",
      "\n",
      "        # Update base model and current model config\n",
      "        self.config.vocab_size = new_num_tokens\n",
      "        base_model.vocab_size = new_num_tokens\n",
      "\n",
      "        # Tie weights again if needed\n",
      "        self.tie_weights()\n",
      "\n",
      "        return model_embeds\n",
      "\n",
      "    def _resize_token_embeddings(self, new_num_tokens):\n",
      "        old_embeddings = self.get_input_embeddings()\n",
      "        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n",
      "        self.set_input_embeddings(new_embeddings)\n",
      "        return self.get_input_embeddings()\n",
      "\n",
      "    def _get_resized_embeddings(self, old_embeddings, new_num_tokens=None):\n",
      "        \"\"\" Build a resized Embedding Module from a provided token Embedding Module.\n",
      "            Increasing the size will add newly initialized vectors at the end\n",
      "            Reducing the size will remove vectors from the end\n",
      "\n",
      "        Args:\n",
      "            new_num_tokens: (`optional`) int\n",
      "                New number of tokens in the embedding matrix.\n",
      "                Increasing the size will add newly initialized vectors at the end\n",
      "                Reducing the size will remove vectors from the end\n",
      "                If not provided or None: return the provided token Embedding Module.\n",
      "        Return: ``torch.nn.Embeddings``\n",
      "            Pointer to the resized Embedding Module or the old Embedding Module if new_num_tokens is None\n",
      "        \"\"\"\n",
      "        if new_num_tokens is None:\n",
      "            return old_embeddings\n",
      "\n",
      "        old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n",
      "        if old_num_tokens == new_num_tokens:\n",
      "            return old_embeddings\n",
      "\n",
      "        # Build new embeddings\n",
      "        new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim)\n",
      "        new_embeddings.to(old_embeddings.weight.device)\n",
      "\n",
      "        # initialize all new embeddings (in particular added tokens)\n",
      "        self._init_weights(new_embeddings)\n",
      "\n",
      "        # Copy token embeddings from the previous weights\n",
      "        num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n",
      "        new_embeddings.weight.data[:num_tokens_to_copy, :] = old_embeddings.weight.data[:num_tokens_to_copy, :]\n",
      "\n",
      "        return new_embeddings\n",
      "\n",
      "    def init_weights(self):\n",
      "        \"\"\" Initialize and prunes weights if needed. \"\"\"\n",
      "        # Initialize weights\n",
      "        self.apply(self._init_weights)\n",
      "\n",
      "        # Prune heads if needed\n",
      "        if self.config.pruned_heads:\n",
      "            self.prune_heads(self.config.pruned_heads)\n",
      "\n",
      "        # Tie weights if needed\n",
      "        self.tie_weights()\n",
      "\n",
      "    def prune_heads(self, heads_to_prune):\n",
      "        \"\"\" Prunes heads of the base model.\n",
      "\n",
      "            Arguments:\n",
      "\n",
      "                heads_to_prune: dict with keys being selected layer indices (`int`) and associated values being the list of heads to prune in said layer (list of `int`).\n",
      "                E.g. {1: [0, 2], 2: [2, 3]} will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.\n",
      "        \"\"\"\n",
      "        # save new sets of pruned heads as union of previously stored pruned heads and newly pruned heads\n",
      "        for layer, heads in heads_to_prune.items():\n",
      "            union_heads = set(self.config.pruned_heads.get(layer, [])) | set(heads)\n",
      "            self.config.pruned_heads[layer] = list(union_heads)  # Unfortunately we have to store it as list for JSON\n",
      "\n",
      "        self.base_model._prune_heads(heads_to_prune)\n",
      "\n",
      "    def save_pretrained(self, save_directory):\n",
      "        \"\"\" Save a model and its configuration file to a directory, so that it\n",
      "            can be re-loaded using the `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.\n",
      "        \"\"\"\n",
      "        assert os.path.isdir(\n",
      "            save_directory\n",
      "        ), \"Saving path should be a directory where the model and configuration can be saved\"\n",
      "\n",
      "        # Only save the model itself if we are using distributed training\n",
      "        model_to_save = self.module if hasattr(self, \"module\") else self\n",
      "\n",
      "        # Attach architecture to the config\n",
      "        model_to_save.config.architectures = [model_to_save.__class__.__name__]\n",
      "\n",
      "        # Save configuration file\n",
      "        model_to_save.config.save_pretrained(save_directory)\n",
      "\n",
      "        # If we save using the predefined names, we can load using `from_pretrained`\n",
      "        output_model_file = os.path.join(save_directory, WEIGHTS_NAME)\n",
      "        torch.save(model_to_save.state_dict(), output_model_file)\n",
      "        logger.info(\"Model weights saved in {}\".format(output_model_file))\n",
      "\n",
      "    @classmethod\n",
      "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
      "        r\"\"\"Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
      "\n",
      "        The model is set in evaluation mode by default using ``model.eval()`` (Dropout modules are deactivated)\n",
      "        To train the model, you should first set it back in training mode with ``model.train()``\n",
      "\n",
      "        The warning ``Weights from XXX not initialized from pretrained model`` means that the weights of XXX do not come pre-trained with the rest of the model.\n",
      "        It is up to you to train those weights with a downstream fine-tuning task.\n",
      "\n",
      "        The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used by YYY, therefore those weights are discarded.\n",
      "\n",
      "        Parameters:\n",
      "            pretrained_model_name_or_path: either:\n",
      "              - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.\n",
      "              - a string with the `identifier name` of a pre-trained model that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.\n",
      "              - a path to a `directory` containing model weights saved using :func:`~transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/``.\n",
      "              - a path or url to a `tensorflow index checkpoint file` (e.g. `./tf_model/model.ckpt.index`). In this case, ``from_tf`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
      "              - None if you are both providing the configuration and state dictionary (resp. with keyword arguments ``config`` and ``state_dict``)\n",
      "\n",
      "            model_args: (`optional`) Sequence of positional arguments:\n",
      "                All remaning positional arguments will be passed to the underlying model's ``__init__`` method\n",
      "\n",
      "            config: (`optional`) one of:\n",
      "                - an instance of a class derived from :class:`~transformers.PretrainedConfig`, or\n",
      "                - a string valid as input to :func:`~transformers.PretrainedConfig.from_pretrained()`\n",
      "                Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n",
      "                    - the model is a model provided by the library (loaded with the ``shortcut-name`` string of a pretrained model), or\n",
      "                    - the model was saved using :func:`~transformers.PreTrainedModel.save_pretrained` and is reloaded by suppling the save directory.\n",
      "                    - the model is loaded by suppling a local directory as ``pretrained_model_name_or_path`` and a configuration JSON file named `config.json` is found in the directory.\n",
      "\n",
      "            state_dict: (`optional`) dict:\n",
      "                an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.\n",
      "                This option can be used if you want to create a model from a pretrained configuration but load your own weights.\n",
      "                In this case though, you should check if using :func:`~transformers.PreTrainedModel.save_pretrained` and :func:`~transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n",
      "\n",
      "            cache_dir: (`optional`) string:\n",
      "                Path to a directory in which a downloaded pre-trained model\n",
      "                configuration should be cached if the standard cache should not be used.\n",
      "\n",
      "            force_download: (`optional`) boolean, default False:\n",
      "                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n",
      "\n",
      "            resume_download: (`optional`) boolean, default False:\n",
      "                Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.\n",
      "\n",
      "            proxies: (`optional`) dict, default None:\n",
      "                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n",
      "                The proxies are used on each request.\n",
      "\n",
      "            output_loading_info: (`optional`) boolean:\n",
      "                Set to ``True`` to also return a dictionnary containing missing keys, unexpected keys and error messages.\n",
      "\n",
      "            kwargs: (`optional`) Remaining dictionary of keyword arguments:\n",
      "                Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ``output_attention=True``). Behave differently depending on whether a `config` is provided or automatically loaded:\n",
      "\n",
      "                - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the underlying model's ``__init__`` method (we assume all relevant updates to the configuration have already been done)\n",
      "                - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class initialization function (:func:`~transformers.PretrainedConfig.from_pretrained`). Each key of ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's ``__init__`` function.\n",
      "\n",
      "        Examples::\n",
      "\n",
      "            # For example purposes. Not runnable.\n",
      "            model = BertModel.from_pretrained('bert-base-uncased')    # Download model and configuration from S3 and cache.\n",
      "            model = BertModel.from_pretrained('./test/saved_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n",
      "            model = BertModel.from_pretrained('bert-base-uncased', output_attention=True)  # Update configuration during loading\n",
      "            assert model.config.output_attention == True\n",
      "            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n",
      "            config = BertConfig.from_json_file('./tf_model/my_tf_model_config.json')\n",
      "            model = BertModel.from_pretrained('./tf_model/my_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n",
      "\n",
      "        \"\"\"\n",
      "        config = kwargs.pop(\"config\", None)\n",
      "        state_dict = kwargs.pop(\"state_dict\", None)\n",
      "        cache_dir = kwargs.pop(\"cache_dir\", None)\n",
      "        from_tf = kwargs.pop(\"from_tf\", False)\n",
      "        force_download = kwargs.pop(\"force_download\", False)\n",
      "        resume_download = kwargs.pop(\"resume_download\", False)\n",
      "        proxies = kwargs.pop(\"proxies\", None)\n",
      "        output_loading_info = kwargs.pop(\"output_loading_info\", False)\n",
      "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
      "\n",
      "        # Load config if we don't provide a configuration\n",
      "        if not isinstance(config, PretrainedConfig):\n",
      "            config_path = config if config is not None else pretrained_model_name_or_path\n",
      "            config, model_kwargs = cls.config_class.from_pretrained(\n",
      "                config_path,\n",
      "                *model_args,\n",
      "                cache_dir=cache_dir,\n",
      "                return_unused_kwargs=True,\n",
      "                force_download=force_download,\n",
      "                resume_download=resume_download,\n",
      "                proxies=proxies,\n",
      "                local_files_only=local_files_only,\n",
      "                **kwargs,\n",
      "            )\n",
      "        else:\n",
      "            model_kwargs = kwargs\n",
      "\n",
      "        # Load model\n",
      "        if pretrained_model_name_or_path is not None:\n",
      "            if pretrained_model_name_or_path in cls.pretrained_model_archive_map:\n",
      "                archive_file = cls.pretrained_model_archive_map[pretrained_model_name_or_path]\n",
      "            elif os.path.isdir(pretrained_model_name_or_path):\n",
      "                if from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")):\n",
      "                    # Load from a TF 1.0 checkpoint\n",
      "                    archive_file = os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")\n",
      "                elif from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)):\n",
      "                    # Load from a TF 2.0 checkpoint\n",
      "                    archive_file = os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)\n",
      "                elif os.path.isfile(os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)):\n",
      "                    # Load from a PyTorch checkpoint\n",
      "                    archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n",
      "                else:\n",
      "                    raise EnvironmentError(\n",
      "                        \"Error no file named {} found in directory {} or `from_tf` set to False\".format(\n",
      "                            [WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + \".index\"],\n",
      "                            pretrained_model_name_or_path,\n",
      "                        )\n",
      "                    )\n",
      "            elif os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n",
      "                archive_file = pretrained_model_name_or_path\n",
      "            elif os.path.isfile(pretrained_model_name_or_path + \".index\"):\n",
      "                assert (\n",
      "                    from_tf\n",
      "                ), \"We found a TensorFlow checkpoint at {}, please set from_tf to True to load from this checkpoint\".format(\n",
      "                    pretrained_model_name_or_path + \".index\"\n",
      "                )\n",
      "                archive_file = pretrained_model_name_or_path + \".index\"\n",
      "            else:\n",
      "                archive_file = hf_bucket_url(\n",
      "                    pretrained_model_name_or_path, postfix=(TF2_WEIGHTS_NAME if from_tf else WEIGHTS_NAME),\n",
      "                )\n",
      "\n",
      "            # redirect to the cache, if necessary\n",
      "            try:\n",
      "                resolved_archive_file = cached_path(\n",
      "                    archive_file,\n",
      "                    cache_dir=cache_dir,\n",
      "                    force_download=force_download,\n",
      "                    proxies=proxies,\n",
      "                    resume_download=resume_download,\n",
      "                    local_files_only=local_files_only,\n",
      "                )\n",
      "            except EnvironmentError:\n",
      "                if pretrained_model_name_or_path in cls.pretrained_model_archive_map:\n",
      "                    msg = \"Couldn't reach server at '{}' to download pretrained weights.\".format(archive_file)\n",
      "                else:\n",
      "                    msg = (\n",
      "                        \"Model name '{}' was not found in model name list ({}). \"\n",
      "                        \"We assumed '{}' was a path or url to model weight files named one of {} but \"\n",
      "                        \"couldn't find any such file at this path or url.\".format(\n",
      "                            pretrained_model_name_or_path,\n",
      "                            \", \".join(cls.pretrained_model_archive_map.keys()),\n",
      "                            archive_file,\n",
      "                            [WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME],\n",
      "                        )\n",
      "                    )\n",
      "                raise EnvironmentError(msg)\n",
      "\n",
      "            if resolved_archive_file == archive_file:\n",
      "                logger.info(\"loading weights file {}\".format(archive_file))\n",
      "            else:\n",
      "                logger.info(\"loading weights file {} from cache at {}\".format(archive_file, resolved_archive_file))\n",
      "        else:\n",
      "            resolved_archive_file = None\n",
      "\n",
      "        # Instantiate model.\n",
      "        model = cls(config, *model_args, **model_kwargs)\n",
      "\n",
      "        if state_dict is None and not from_tf:\n",
      "            try:\n",
      "                state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n",
      "            except Exception:\n",
      "                raise OSError(\n",
      "                    \"Unable to load weights from pytorch checkpoint file. \"\n",
      "                    \"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. \"\n",
      "                )\n",
      "\n",
      "        missing_keys = []\n",
      "        unexpected_keys = []\n",
      "        error_msgs = []\n",
      "\n",
      "        if from_tf:\n",
      "            if resolved_archive_file.endswith(\".index\"):\n",
      "                # Load from a TensorFlow 1.X checkpoint - provided by original authors\n",
      "                model = cls.load_tf_weights(model, config, resolved_archive_file[:-6])  # Remove the '.index'\n",
      "            else:\n",
      "                # Load from our TensorFlow 2.0 checkpoints\n",
      "                try:\n",
      "                    from transformers import load_tf2_checkpoint_in_pytorch_model\n",
      "\n",
      "                    model = load_tf2_checkpoint_in_pytorch_model(model, resolved_archive_file, allow_missing_keys=True)\n",
      "                except ImportError:\n",
      "                    logger.error(\n",
      "                        \"Loading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed. Please see \"\n",
      "                        \"https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\"\n",
      "                    )\n",
      "                    raise\n",
      "        else:\n",
      "            # Convert old format to new format if needed from a PyTorch state_dict\n",
      "            old_keys = []\n",
      "            new_keys = []\n",
      "            for key in state_dict.keys():\n",
      "                new_key = None\n",
      "                if \"gamma\" in key:\n",
      "                    new_key = key.replace(\"gamma\", \"weight\")\n",
      "                if \"beta\" in key:\n",
      "                    new_key = key.replace(\"beta\", \"bias\")\n",
      "                if new_key:\n",
      "                    old_keys.append(key)\n",
      "                    new_keys.append(new_key)\n",
      "            for old_key, new_key in zip(old_keys, new_keys):\n",
      "                state_dict[new_key] = state_dict.pop(old_key)\n",
      "\n",
      "            # copy state_dict so _load_from_state_dict can modify it\n",
      "            metadata = getattr(state_dict, \"_metadata\", None)\n",
      "            state_dict = state_dict.copy()\n",
      "            if metadata is not None:\n",
      "                state_dict._metadata = metadata\n",
      "\n",
      "            # PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants\n",
      "            # so we need to apply the function recursively.\n",
      "            def load(module: nn.Module, prefix=\"\"):\n",
      "                local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
      "                module._load_from_state_dict(\n",
      "                    state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs,\n",
      "                )\n",
      "                for name, child in module._modules.items():\n",
      "                    if child is not None:\n",
      "                        load(child, prefix + name + \".\")\n",
      "\n",
      "            # Make sure we are able to load base models as well as derived models (with heads)\n",
      "            start_prefix = \"\"\n",
      "            model_to_load = model\n",
      "            if not hasattr(model, cls.base_model_prefix) and any(\n",
      "                s.startswith(cls.base_model_prefix) for s in state_dict.keys()\n",
      "            ):\n",
      "                start_prefix = cls.base_model_prefix + \".\"\n",
      "            if hasattr(model, cls.base_model_prefix) and not any(\n",
      "                s.startswith(cls.base_model_prefix) for s in state_dict.keys()\n",
      "            ):\n",
      "                model_to_load = getattr(model, cls.base_model_prefix)\n",
      "\n",
      "            load(model_to_load, prefix=start_prefix)\n",
      "\n",
      "            if model.__class__.__name__ != model_to_load.__class__.__name__:\n",
      "                base_model_state_dict = model_to_load.state_dict().keys()\n",
      "                head_model_state_dict_without_base_prefix = [\n",
      "                    key.split(cls.base_model_prefix + \".\")[-1] for key in model.state_dict().keys()\n",
      "                ]\n",
      "\n",
      "                missing_keys.extend(head_model_state_dict_without_base_prefix - base_model_state_dict)\n",
      "\n",
      "            if len(missing_keys) > 0:\n",
      "                logger.info(\n",
      "                    \"Weights of {} not initialized from pretrained model: {}\".format(\n",
      "                        model.__class__.__name__, missing_keys\n",
      "                    )\n",
      "                )\n",
      "            if len(unexpected_keys) > 0:\n",
      "                logger.info(\n",
      "                    \"Weights from pretrained model not used in {}: {}\".format(\n",
      "                        model.__class__.__name__, unexpected_keys\n",
      "                    )\n",
      "                )\n",
      "            if len(error_msgs) > 0:\n",
      "                raise RuntimeError(\n",
      "                    \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n",
      "                        model.__class__.__name__, \"\\n\\t\".join(error_msgs)\n",
      "                    )\n",
      "                )\n",
      "        model.tie_weights()  # make sure token embedding weights are still tied if needed\n",
      "\n",
      "        # Set model in evaluation mode to desactivate DropOut modules by default\n",
      "        model.eval()\n",
      "\n",
      "        if output_loading_info:\n",
      "            loading_info = {\n",
      "                \"missing_keys\": missing_keys,\n",
      "                \"unexpected_keys\": unexpected_keys,\n",
      "                \"error_msgs\": error_msgs,\n",
      "            }\n",
      "            return model, loading_info\n",
      "\n",
      "        return model\n",
      "\n",
      "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
      "        return {\"input_ids\": input_ids}\n",
      "\n",
      "    def prepare_scores_for_generation(self, scores, **kwargs):\n",
      "        return scores\n",
      "\n",
      "    def _do_output_past(self, outputs):\n",
      "        \"\"\"During generation, decide whether to pass the `past` variable to the next forward pass.\"\"\"\n",
      "        has_output_past = getattr(self.config, \"output_past\", False)\n",
      "        mem_len = getattr(self.config, \"mem_len\", 0)\n",
      "        if len(outputs) <= 1:\n",
      "            return False\n",
      "        if mem_len > 0 or has_output_past:\n",
      "            return True\n",
      "        return False\n",
      "\n",
      "    def enforce_repetition_penalty_(self, lprobs, batch_size, num_beams, prev_output_tokens, repetition_penalty):\n",
      "        \"\"\"repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858). \"\"\"\n",
      "        for i in range(batch_size * num_beams):\n",
      "            for previous_token in set(prev_output_tokens[i].tolist()):\n",
      "                # if score < 0 then repetition penalty has to multiplied to reduce the previous token probability\n",
      "                if lprobs[i, previous_token] < 0:\n",
      "                    lprobs[i, previous_token] *= repetition_penalty\n",
      "                else:\n",
      "                    lprobs[i, previous_token] /= repetition_penalty\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def generate(\n",
      "        self,\n",
      "        input_ids=None,\n",
      "        max_length=None,\n",
      "        min_length=None,\n",
      "        do_sample=None,\n",
      "        early_stopping=None,\n",
      "        num_beams=None,\n",
      "        temperature=None,\n",
      "        top_k=None,\n",
      "        top_p=None,\n",
      "        repetition_penalty=None,\n",
      "        bos_token_id=None,\n",
      "        pad_token_id=None,\n",
      "        eos_token_id=None,\n",
      "        length_penalty=None,\n",
      "        no_repeat_ngram_size=None,\n",
      "        num_return_sequences=None,\n",
      "        attention_mask=None,\n",
      "        decoder_start_token_id=None,\n",
      "    ):\n",
      "        r\"\"\" Generates sequences for models with a LM head. The method currently supports greedy decoding, beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.\n",
      "\n",
      "        Adapted in part from `Facebook's XLM beam search code`_.\n",
      "\n",
      "        .. _`Facebook's XLM beam search code`:\n",
      "           https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529\n",
      "\n",
      "\n",
      "        Parameters:\n",
      "\n",
      "            input_ids: (`optional`) `torch.LongTensor` of shape `(batch_size, sequence_length)`\n",
      "                The sequence used as a prompt for the generation. If `None` the method initializes\n",
      "                it as an empty `torch.LongTensor` of shape `(1,)`.\n",
      "\n",
      "            max_length: (`optional`) int\n",
      "                The max length of the sequence to be generated.  Between `min_length` and infinity. Default to 20.\n",
      "\n",
      "            min_length: (`optional`) int\n",
      "                The min length of the sequence to be generated.  Between 0 and infinity. Default to 0.\n",
      "\n",
      "            do_sample: (`optional`) bool\n",
      "                If set to `False` greedy decoding is used. Otherwise sampling is used. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.\n",
      "\n",
      "            early_stopping: (`optional`) bool\n",
      "                if set to `True` beam search is stopped when at least `num_beams` sentences finished per batch. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.\n",
      "\n",
      "            num_beams: (`optional`) int\n",
      "                Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.\n",
      "\n",
      "            temperature: (`optional`) float\n",
      "                The value used to module the next token probabilities. Must be strictly positive. Default to 1.0.\n",
      "\n",
      "            top_k: (`optional`) int\n",
      "                The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.\n",
      "\n",
      "            top_p: (`optional`) float\n",
      "                The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.\n",
      "\n",
      "            repetition_penalty: (`optional`) float\n",
      "                The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.\n",
      "\n",
      "            pad_token_id: (`optional`) int\n",
      "                Padding token. Default to specicic model pad_token_id or None if it does not exist.\n",
      "\n",
      "            bos_token_id: (`optional`) int\n",
      "                BOS token. Defaults to bos_token_id as defined in the models config.\n",
      "\n",
      "            pad_token_id: (`optional`) int\n",
      "                Pad token. Defaults to pad_token_id as defined in the models config.\n",
      "\n",
      "            eos_token_ids: (`optional`) int or list of int\n",
      "                End of sequence token or list of tokens to stop the generation. Default to eos_token_ids as defined in the models config.\n",
      "\n",
      "            length_penalty: (`optional`) float\n",
      "                Exponential penalty to the length. Default to 1.\n",
      "\n",
      "            no_repeat_ngram_size: (`optional`) int\n",
      "                If set to int > 0, all ngrams of size `no_repeat_ngram_size` can only occur once.\n",
      "\n",
      "            num_return_sequences: (`optional`) int\n",
      "                The number of independently computed returned sequences for each element in the batch. Default to 1.\n",
      "\n",
      "            attention_mask (`optional`) obj: `torch.LongTensor` of same shape as `input_ids`\n",
      "                Mask to avoid performing attention on padding token indices.\n",
      "                Mask values selected in ``[0, 1]``:\n",
      "                ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n",
      "                Defaults to `None`.\n",
      "\n",
      "            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      "\n",
      "            decoder_start_token_id=None: (`optional`) int\n",
      "                If an encoder-decoder model starts decoding with a different token than BOS.\n",
      "                Defaults to `None` and is changed to `BOS` later.\n",
      "\n",
      "        Return:\n",
      "\n",
      "            output: `torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`\n",
      "                sequence_length is either equal to max_length or shorter if all batches finished early due to the `eos_token_id`\n",
      "\n",
      "        Examples::\n",
      "\n",
      "            tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n",
      "            model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\n",
      "            outputs = model.generate(max_length=40)  # do greedy decoding\n",
      "            print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n",
      "\n",
      "            tokenizer = AutoTokenizer.from_pretrained('openai-gpt')   # Initialize tokenizer\n",
      "            model = AutoModelWithLMHead.from_pretrained('openai-gpt')    # Download model and configuration from S3 and cache.\n",
      "            input_context = 'The dog'\n",
      "            input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n",
      "            outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3, temperature=1.5)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'\n",
      "            for i in range(3): #  3 output sequences were generated\n",
      "                print('Generated {}: {}'.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))\n",
      "\n",
      "            tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n",
      "            model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\n",
      "            input_context = 'The dog'\n",
      "            input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n",
      "            outputs = model.generate(input_ids=input_ids, max_length=40, temperature=0.7, num_return_sequences=3)  # 3 generate sequences using by sampling\n",
      "            for i in range(3): #  3 output sequences were generated\n",
      "                print('Generated {}: {}'.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))\n",
      "\n",
      "            tokenizer = AutoTokenizer.from_pretrained('ctrl')   # Initialize tokenizer\n",
      "            model = AutoModelWithLMHead.from_pretrained('ctrl')    # Download model and configuration from S3 and cache.\n",
      "            input_context = 'Legal My neighbor is'  # \"Legal\" is one of the control codes for ctrl\n",
      "            input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n",
      "            outputs = model.generate(input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2)  # generate sequences\n",
      "            print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        # We cannot generate if the model does not have a LM head\n",
      "        if self.get_output_embeddings() is None:\n",
      "            raise AttributeError(\n",
      "                \"You tried to generate sequences with a model that does not have a LM Head.\"\n",
      "                \"Please use another model class (e.g. `OpenAIGPTLMHeadModel`, `XLNetLMHeadModel`, `GPT2LMHeadModel`, `CTRLLMHeadModel`, `T5WithLMHeadModel`, `TransfoXLLMHeadModel`, `XLMWithLMHeadModel`, `BartForConditionalGeneration` )\"\n",
      "            )\n",
      "\n",
      "        max_length = max_length if max_length is not None else self.config.max_length\n",
      "        min_length = min_length if min_length is not None else self.config.min_length\n",
      "        do_sample = do_sample if do_sample is not None else self.config.do_sample\n",
      "        early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping\n",
      "        num_beams = num_beams if num_beams is not None else self.config.num_beams\n",
      "        temperature = temperature if temperature is not None else self.config.temperature\n",
      "        top_k = top_k if top_k is not None else self.config.top_k\n",
      "        top_p = top_p if top_p is not None else self.config.top_p\n",
      "        repetition_penalty = repetition_penalty if repetition_penalty is not None else self.config.repetition_penalty\n",
      "        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id\n",
      "        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n",
      "        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n",
      "        length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n",
      "        no_repeat_ngram_size = (\n",
      "            no_repeat_ngram_size if no_repeat_ngram_size is not None else self.config.no_repeat_ngram_size\n",
      "        )\n",
      "        num_return_sequences = (\n",
      "            num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences\n",
      "        )\n",
      "        decoder_start_token_id = decoder_start_token_id if decoder_start_token_id is not None else bos_token_id\n",
      "\n",
      "        if input_ids is not None:\n",
      "            batch_size = input_ids.shape[0]  # overriden by the input batch_size\n",
      "        else:\n",
      "            batch_size = 1\n",
      "\n",
      "        assert isinstance(max_length, int) and max_length > 0, \"`max_length` should be a strictly positive integer.\"\n",
      "        assert isinstance(min_length, int) and min_length >= 0, \"`min_length` should be a positive integer.\"\n",
      "        assert isinstance(do_sample, bool), \"`do_sample` should be a boolean.\"\n",
      "        assert isinstance(early_stopping, bool), \"`early_stopping` should be a boolean.\"\n",
      "        assert isinstance(num_beams, int) and num_beams > 0, \"`num_beams` should be a strictly positive integer.\"\n",
      "        assert temperature > 0, \"`temperature` should be strictly positive.\"\n",
      "        assert isinstance(top_k, int) and top_k >= 0, \"`top_k` should be a positive integer.\"\n",
      "        assert 0 <= top_p <= 1, \"`top_p` should be between 0 and 1.\"\n",
      "        assert repetition_penalty >= 1.0, \"`repetition_penalty` should be >= 1.\"\n",
      "        assert input_ids is not None or (\n",
      "            isinstance(bos_token_id, int) and bos_token_id >= 0\n",
      "        ), \"If input_ids is not defined, `bos_token_id` should be a positive integer.\"\n",
      "        assert pad_token_id is None or (\n",
      "            isinstance(pad_token_id, int) and (pad_token_id >= 0)\n",
      "        ), \"`pad_token_id` should be a positive integer.\"\n",
      "        assert (\n",
      "            decoder_start_token_id is not None or self.config.is_encoder_decoder is False\n",
      "        ), \"`decoder_start_token_id` has to be defined if model is encoder-decoder model\"\n",
      "        assert (eos_token_id is None) or (\n",
      "            isinstance(eos_token_id, int) and (eos_token_id >= 0)\n",
      "        ), \"`eos_token_id` should be a positive integer.\"\n",
      "        assert length_penalty > 0, \"`length_penalty` should be strictly positive.\"\n",
      "        assert (\n",
      "            isinstance(no_repeat_ngram_size, int) and no_repeat_ngram_size >= 0\n",
      "        ), \"`no_repeat_ngram_size` should be a positive integer.\"\n",
      "        assert (\n",
      "            isinstance(num_return_sequences, int) and num_return_sequences > 0\n",
      "        ), \"`num_return_sequences` should be a strictly positive integer.\"\n",
      "\n",
      "        if input_ids is None:\n",
      "            assert isinstance(bos_token_id, int) and bos_token_id >= 0, (\n",
      "                \"you should either supply a context to complete as `input_ids` input \"\n",
      "                \"or a `bos_token_id` (integer >= 0) as a first token to start the generation.\"\n",
      "            )\n",
      "            input_ids = torch.full(\n",
      "                (batch_size, 1), bos_token_id, dtype=torch.long, device=next(self.parameters()).device,\n",
      "            )\n",
      "        else:\n",
      "            assert input_ids.dim() == 2, \"Input prompt should be of shape (batch_size, sequence length).\"\n",
      "\n",
      "        # not allow to duplicate outputs when greedy decoding\n",
      "        if do_sample is False:\n",
      "            if num_beams == 1:\n",
      "                # no_beam_search greedy generation conditions\n",
      "                assert (\n",
      "                    num_return_sequences == 1\n",
      "                ), \"Greedy decoding will always produce the same output for num_beams == 1 and num_return_sequences > 1. Please set num_return_sequences = 1\"\n",
      "\n",
      "            else:\n",
      "                # beam_search greedy generation conditions\n",
      "                assert (\n",
      "                    num_beams >= num_return_sequences\n",
      "                ), \"Greedy beam search decoding cannot return more sequences than it has beams. Please set num_beams >= num_return_sequences\"\n",
      "\n",
      "        # create attention mask if necessary\n",
      "        # TODO (PVP): this should later be handled by the forward fn() in each model in the future see PR 3140\n",
      "        if (attention_mask is None) and (pad_token_id is not None) and (pad_token_id in input_ids):\n",
      "            attention_mask = input_ids.ne(pad_token_id).long()\n",
      "        elif attention_mask is None:\n",
      "            attention_mask = input_ids.new_ones(input_ids.shape)\n",
      "\n",
      "        # set pad_token_id to eos_token_id if not set. Important that this is done after\n",
      "        # attention_mask is created\n",
      "        if pad_token_id is None and eos_token_id is not None:\n",
      "            logger.warning(\n",
      "                \"Setting `pad_token_id` to {} (first `eos_token_id`) to generate sequence\".format(eos_token_id)\n",
      "            )\n",
      "            pad_token_id = eos_token_id\n",
      "\n",
      "        # current position and vocab size\n",
      "        vocab_size = self.config.vocab_size\n",
      "\n",
      "        # set effective batch size and effective batch multiplier according to do_sample\n",
      "        if do_sample:\n",
      "            effective_batch_size = batch_size * num_return_sequences\n",
      "            effective_batch_mult = num_return_sequences\n",
      "        else:\n",
      "            effective_batch_size = batch_size\n",
      "            effective_batch_mult = 1\n",
      "\n",
      "        # Expand input ids if num_beams > 1 or num_return_sequences > 1\n",
      "        if num_return_sequences > 1 or num_beams > 1:\n",
      "            input_ids_len = input_ids.shape[-1]\n",
      "            input_ids = input_ids.unsqueeze(1).expand(batch_size, effective_batch_mult * num_beams, input_ids_len)\n",
      "            attention_mask = attention_mask.unsqueeze(1).expand(\n",
      "                batch_size, effective_batch_mult * num_beams, input_ids_len\n",
      "            )\n",
      "\n",
      "            input_ids = input_ids.contiguous().view(\n",
      "                effective_batch_size * num_beams, input_ids_len\n",
      "            )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)\n",
      "            attention_mask = attention_mask.contiguous().view(\n",
      "                effective_batch_size * num_beams, input_ids_len\n",
      "            )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)\n",
      "\n",
      "        if self.config.is_encoder_decoder:\n",
      "            assert bos_token_id is not None, \"Encoder Decoder Models need to have a bos_token_id\"\n",
      "            assert hasattr(self, \"get_encoder\"), \"{} should have a 'get_encoder' function defined\".format(self)\n",
      "            assert callable(self.get_encoder), \"{} should be a method\".format(self.get_encoder)\n",
      "\n",
      "            # get encoder and store encoder outputs\n",
      "            encoder = self.get_encoder()\n",
      "\n",
      "            encoder_outputs = encoder(input_ids, attention_mask=attention_mask)\n",
      "\n",
      "            # create empty decoder_input_ids\n",
      "            input_ids = torch.full(\n",
      "                (effective_batch_size * num_beams, 1),\n",
      "                decoder_start_token_id,\n",
      "                dtype=torch.long,\n",
      "                device=next(self.parameters()).device,\n",
      "            )\n",
      "            cur_len = 1\n",
      "        else:\n",
      "            encoder_outputs = None\n",
      "            cur_len = input_ids.shape[-1]\n",
      "\n",
      "        if num_beams > 1:\n",
      "            output = self._generate_beam_search(\n",
      "                input_ids,\n",
      "                cur_len=cur_len,\n",
      "                max_length=max_length,\n",
      "                min_length=min_length,\n",
      "                do_sample=do_sample,\n",
      "                early_stopping=early_stopping,\n",
      "                temperature=temperature,\n",
      "                top_k=top_k,\n",
      "                top_p=top_p,\n",
      "                repetition_penalty=repetition_penalty,\n",
      "                no_repeat_ngram_size=no_repeat_ngram_size,\n",
      "                bos_token_id=bos_token_id,\n",
      "                pad_token_id=pad_token_id,\n",
      "                decoder_start_token_id=decoder_start_token_id,\n",
      "                eos_token_id=eos_token_id,\n",
      "                batch_size=effective_batch_size,\n",
      "                num_return_sequences=num_return_sequences,\n",
      "                length_penalty=length_penalty,\n",
      "                num_beams=num_beams,\n",
      "                vocab_size=vocab_size,\n",
      "                encoder_outputs=encoder_outputs,\n",
      "                attention_mask=attention_mask,\n",
      "            )\n",
      "        else:\n",
      "            output = self._generate_no_beam_search(\n",
      "                input_ids,\n",
      "                cur_len=cur_len,\n",
      "                max_length=max_length,\n",
      "                min_length=min_length,\n",
      "                do_sample=do_sample,\n",
      "                temperature=temperature,\n",
      "                top_k=top_k,\n",
      "                top_p=top_p,\n",
      "                repetition_penalty=repetition_penalty,\n",
      "                no_repeat_ngram_size=no_repeat_ngram_size,\n",
      "                bos_token_id=bos_token_id,\n",
      "                pad_token_id=pad_token_id,\n",
      "                decoder_start_token_id=decoder_start_token_id,\n",
      "                eos_token_id=eos_token_id,\n",
      "                batch_size=effective_batch_size,\n",
      "                encoder_outputs=encoder_outputs,\n",
      "                attention_mask=attention_mask,\n",
      "            )\n",
      "\n",
      "        return output\n",
      "\n",
      "    def _generate_no_beam_search(\n",
      "        self,\n",
      "        input_ids,\n",
      "        cur_len,\n",
      "        max_length,\n",
      "        min_length,\n",
      "        do_sample,\n",
      "        temperature,\n",
      "        top_k,\n",
      "        top_p,\n",
      "        repetition_penalty,\n",
      "        no_repeat_ngram_size,\n",
      "        bos_token_id,\n",
      "        pad_token_id,\n",
      "        eos_token_id,\n",
      "        decoder_start_token_id,\n",
      "        batch_size,\n",
      "        encoder_outputs,\n",
      "        attention_mask,\n",
      "    ):\n",
      "        \"\"\" Generate sequences for each example without beam search (num_beams == 1).\n",
      "            All returned sequence are generated independantly.\n",
      "        \"\"\"\n",
      "        # length of generated sentences / unfinished sentences\n",
      "        unfinished_sents = input_ids.new(batch_size).fill_(1)\n",
      "        sent_lengths = input_ids.new(batch_size).fill_(max_length)\n",
      "\n",
      "        past = encoder_outputs  # defined for encoder-decoder models, None for decoder-only models\n",
      "\n",
      "        while cur_len < max_length:\n",
      "            model_inputs = self.prepare_inputs_for_generation(input_ids, past=past, attention_mask=attention_mask)\n",
      "\n",
      "            outputs = self(**model_inputs)\n",
      "            next_token_logits = outputs[0][:, -1, :]\n",
      "\n",
      "            # if model has past, then set the past variable to speed up decoding\n",
      "            if self._do_output_past(outputs):\n",
      "                past = outputs[1]\n",
      "\n",
      "            # repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)\n",
      "            if repetition_penalty != 1.0:\n",
      "                self.enforce_repetition_penalty_(next_token_logits, batch_size, 1, input_ids, repetition_penalty)\n",
      "\n",
      "            if no_repeat_ngram_size > 0:\n",
      "                # calculate a list of banned tokens to prevent repetitively generating the same ngrams\n",
      "                # from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345\n",
      "                banned_tokens = calc_banned_tokens(input_ids, batch_size, no_repeat_ngram_size, cur_len)\n",
      "                for batch_idx in range(batch_size):\n",
      "                    next_token_logits[batch_idx, banned_tokens[batch_idx]] = -float(\"inf\")\n",
      "\n",
      "            # set eos token prob to zero if min_length is not reached\n",
      "            if eos_token_id is not None and cur_len < min_length:\n",
      "                next_token_logits[:, eos_token_id] = -float(\"inf\")\n",
      "\n",
      "            if do_sample:\n",
      "                # Temperature (higher temperature => more likely to sample low probability tokens)\n",
      "                if temperature != 1.0:\n",
      "                    next_token_logits = next_token_logits / temperature\n",
      "                # Top-p/top-k filtering\n",
      "                next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
      "                # Sample\n",
      "                probs = F.softmax(next_token_logits, dim=-1)\n",
      "                next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
      "            else:\n",
      "                # Greedy decoding\n",
      "                next_token = torch.argmax(next_token_logits, dim=-1)\n",
      "\n",
      "            # update generations and finished sentences\n",
      "            if eos_token_id is not None:\n",
      "                # pad finished sentences if eos_token_id exist\n",
      "                tokens_to_add = next_token * unfinished_sents + (pad_token_id) * (1 - unfinished_sents)\n",
      "            else:\n",
      "                tokens_to_add = next_token\n",
      "\n",
      "            input_ids = torch.cat([input_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
      "\n",
      "            if eos_token_id is not None:\n",
      "                eos_in_sents = tokens_to_add == eos_token_id\n",
      "                # if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length\n",
      "                is_sents_unfinished_and_token_to_add_is_eos = unfinished_sents.mul(eos_in_sents.long()).bool()\n",
      "                sent_lengths.masked_fill_(is_sents_unfinished_and_token_to_add_is_eos, cur_len + 1)\n",
      "                # unfinished_sents is set to zero if eos in sentence\n",
      "                unfinished_sents.mul_((~eos_in_sents).long())\n",
      "\n",
      "            # stop when there is a </s> in each sentence, or if we exceed the maximul length\n",
      "            if unfinished_sents.max() == 0:\n",
      "                break\n",
      "\n",
      "            # extend attention_mask for new generated input if only decoder\n",
      "            if self.config.is_encoder_decoder is False:\n",
      "                attention_mask = torch.cat(\n",
      "                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n",
      "                )\n",
      "\n",
      "            cur_len = cur_len + 1\n",
      "\n",
      "        # if there are different sentences lengths in the batch, some batches have to be padded\n",
      "        if sent_lengths.min().item() != sent_lengths.max().item():\n",
      "            assert pad_token_id is not None, \"`Pad_token_id` has to be defined if batches have different lengths\"\n",
      "            # finished sents are filled with pad_token\n",
      "            decoded = input_ids.new(batch_size, sent_lengths.max().item()).fill_(pad_token_id)\n",
      "        else:\n",
      "            decoded = input_ids\n",
      "\n",
      "        for hypo_idx, hypo in enumerate(input_ids):\n",
      "            decoded[hypo_idx, : sent_lengths[hypo_idx]] = hypo[: sent_lengths[hypo_idx]]\n",
      "\n",
      "        return decoded\n",
      "\n",
      "    def _generate_beam_search(\n",
      "        self,\n",
      "        input_ids,\n",
      "        cur_len,\n",
      "        max_length,\n",
      "        min_length,\n",
      "        do_sample,\n",
      "        early_stopping,\n",
      "        temperature,\n",
      "        top_k,\n",
      "        top_p,\n",
      "        repetition_penalty,\n",
      "        no_repeat_ngram_size,\n",
      "        bos_token_id,\n",
      "        pad_token_id,\n",
      "        eos_token_id,\n",
      "        decoder_start_token_id,\n",
      "        batch_size,\n",
      "        num_return_sequences,\n",
      "        length_penalty,\n",
      "        num_beams,\n",
      "        vocab_size,\n",
      "        encoder_outputs,\n",
      "        attention_mask,\n",
      "    ):\n",
      "        \"\"\" Generate sequences for each example with beam search.\n",
      "        \"\"\"\n",
      "\n",
      "        # generated hypotheses\n",
      "        generated_hyps = [\n",
      "            BeamHypotheses(num_beams, max_length, length_penalty, early_stopping=early_stopping)\n",
      "            for _ in range(batch_size)\n",
      "        ]\n",
      "\n",
      "        # scores for each sentence in the beam\n",
      "        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
      "\n",
      "        # for greedy decoding it is made sure that only tokens of the first beam are considered to avoid sampling the exact same tokens three times\n",
      "        if do_sample is False:\n",
      "            beam_scores[:, 1:] = -1e9\n",
      "        beam_scores = beam_scores.view(-1)  # shape (batch_size * num_beams,)\n",
      "\n",
      "        # cache compute states\n",
      "        past = encoder_outputs  # defined for encoder-decoder models, None for decoder-only models\n",
      "\n",
      "        # done sentences\n",
      "        done = [False for _ in range(batch_size)]\n",
      "\n",
      "        while cur_len < max_length:\n",
      "            model_inputs = self.prepare_inputs_for_generation(input_ids, past=past, attention_mask=attention_mask)\n",
      "            outputs = self(**model_inputs)  # (batch_size * num_beams, cur_len, vocab_size)\n",
      "            next_token_logits = outputs[0][:, -1, :]  # (batch_size * num_beams, vocab_size)\n",
      "\n",
      "            # if model has past, then set the past variable to speed up decoding\n",
      "            if self._do_output_past(outputs):\n",
      "                past = outputs[1]\n",
      "\n",
      "            # repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)\n",
      "            if repetition_penalty != 1.0:\n",
      "                self.enforce_repetition_penalty_(\n",
      "                    next_token_logits, batch_size, num_beams, input_ids, repetition_penalty,\n",
      "                )\n",
      "\n",
      "            if temperature != 1.0:\n",
      "                next_token_logits = next_token_logits / temperature\n",
      "\n",
      "            scores = F.log_softmax(next_token_logits, dim=-1)  # (batch_size * num_beams, vocab_size)\n",
      "            if self.config.is_encoder_decoder and do_sample is False:\n",
      "                # TODO (PVP) still a bit hacky here - there might be a better solutino\n",
      "                scores = self.prepare_scores_for_generation(scores, cur_len=cur_len, max_length=max_length)\n",
      "\n",
      "            # set eos token prob to zero if min_length is not reached\n",
      "            if eos_token_id is not None and cur_len < min_length:\n",
      "                scores[:, eos_token_id] = -float(\"inf\")\n",
      "\n",
      "            if no_repeat_ngram_size > 0:\n",
      "                # calculate a list of banned tokens to prevent repetitively generating the same ngrams\n",
      "                num_batch_hypotheses = batch_size * num_beams\n",
      "                # from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345\n",
      "                banned_batch_tokens = calc_banned_tokens(\n",
      "                    input_ids, num_batch_hypotheses, no_repeat_ngram_size, cur_len\n",
      "                )\n",
      "                for i, banned_tokens in enumerate(banned_batch_tokens):\n",
      "                    scores[i, banned_tokens] = -float(\"inf\")\n",
      "\n",
      "            assert scores.shape == (batch_size * num_beams, vocab_size), \"Shapes of scores: {} != {}\".format(\n",
      "                scores.shape, (batch_size * num_beams, vocab_size)\n",
      "            )\n",
      "\n",
      "            if do_sample:\n",
      "                _scores = scores + beam_scores[:, None].expand_as(scores)  # (batch_size * num_beams, vocab_size)\n",
      "                # Top-p/top-k filtering\n",
      "                _scores = top_k_top_p_filtering(\n",
      "                    _scores, top_k=top_k, top_p=top_p, min_tokens_to_keep=2\n",
      "                )  # (batch_size * num_beams, vocab_size)\n",
      "                # re-organize to group the beam together to sample from all beam_idxs\n",
      "                _scores = _scores.contiguous().view(\n",
      "                    batch_size, num_beams * vocab_size\n",
      "                )  # (batch_size, num_beams * vocab_size)\n",
      "\n",
      "                # Sample 2 next tokens for each beam (so we have some spare tokens and match output of greedy beam search)\n",
      "                probs = F.softmax(_scores, dim=-1)\n",
      "                next_tokens = torch.multinomial(probs, num_samples=2 * num_beams)  # (batch_size, num_beams * 2)\n",
      "                # Compute next scores\n",
      "                next_scores = torch.gather(_scores, -1, next_tokens)  # (batch_size, num_beams * 2)\n",
      "                # sort the sampled vector to make sure that the first num_beams samples are the best\n",
      "                next_scores, next_scores_indices = torch.sort(next_scores, descending=True, dim=1)\n",
      "                next_tokens = torch.gather(next_tokens, -1, next_scores_indices)  # (batch_size, num_beams * 2)\n",
      "\n",
      "            else:\n",
      "                next_scores = scores + beam_scores[:, None].expand_as(scores)  # (batch_size * num_beams, vocab_size)\n",
      "\n",
      "                # re-organize to group the beam together (we are keeping top hypothesis accross beams)\n",
      "                next_scores = next_scores.view(\n",
      "                    batch_size, num_beams * vocab_size\n",
      "                )  # (batch_size, num_beams * vocab_size)\n",
      "\n",
      "                next_scores, next_tokens = torch.topk(next_scores, 2 * num_beams, dim=1, largest=True, sorted=True)\n",
      "\n",
      "            assert next_scores.size() == next_tokens.size() == (batch_size, 2 * num_beams)\n",
      "\n",
      "            # next batch beam content\n",
      "            next_batch_beam = []\n",
      "\n",
      "            # for each sentence\n",
      "            for batch_idx in range(batch_size):\n",
      "\n",
      "                # if we are done with this sentence\n",
      "                if done[batch_idx]:\n",
      "                    assert (\n",
      "                        len(generated_hyps[batch_idx]) >= num_beams\n",
      "                    ), \"Batch can only be done if at least {} beams have been generated\".format(num_beams)\n",
      "                    assert (\n",
      "                        eos_token_id is not None and pad_token_id is not None\n",
      "                    ), \"generated beams >= num_beams -> eos_token_id and pad_token have to be defined\"\n",
      "                    next_batch_beam.extend([(0, pad_token_id, 0)] * num_beams)  # pad the batch\n",
      "                    continue\n",
      "\n",
      "                # next sentence beam content\n",
      "                next_sent_beam = []\n",
      "\n",
      "                # next tokens for this sentence\n",
      "                for beam_token_rank, (beam_token_id, beam_token_score) in enumerate(\n",
      "                    zip(next_tokens[batch_idx], next_scores[batch_idx])\n",
      "                ):\n",
      "                    # get beam and word IDs\n",
      "                    beam_id = beam_token_id // vocab_size\n",
      "                    token_id = beam_token_id % vocab_size\n",
      "\n",
      "                    effective_beam_id = batch_idx * num_beams + beam_id\n",
      "\n",
      "                    # add to generated hypotheses if end of sentence\n",
      "                    if (eos_token_id is not None) and (token_id.item() is eos_token_id):\n",
      "                        # if beam_token does not belong to top num_beams tokens, it should not be added\n",
      "                        is_beam_token_worse_than_top_num_beams = beam_token_rank >= num_beams\n",
      "                        if is_beam_token_worse_than_top_num_beams:\n",
      "                            continue\n",
      "                        generated_hyps[batch_idx].add(\n",
      "                            input_ids[effective_beam_id].clone(), beam_token_score.item(),\n",
      "                        )\n",
      "                    else:\n",
      "                        # add next predicted word if it is not eos_token\n",
      "                        next_sent_beam.append((beam_token_score, token_id, effective_beam_id))\n",
      "\n",
      "                    # the beam for next step is full\n",
      "                    if len(next_sent_beam) == num_beams:\n",
      "                        break\n",
      "\n",
      "                # Check if were done so that we can save a pad step if all(done)\n",
      "                done[batch_idx] = done[batch_idx] or generated_hyps[batch_idx].is_done(\n",
      "                    next_scores[batch_idx].max().item(), cur_len=cur_len\n",
      "                )\n",
      "\n",
      "                # update next beam content\n",
      "                assert len(next_sent_beam) == num_beams, \"Beam should always be full\"\n",
      "                next_batch_beam.extend(next_sent_beam)\n",
      "                assert len(next_batch_beam) == num_beams * (batch_idx + 1)\n",
      "\n",
      "            # stop when we are done with each sentence\n",
      "            if all(done):\n",
      "                break\n",
      "\n",
      "            # sanity check / prepare next batch\n",
      "            assert len(next_batch_beam) == batch_size * num_beams\n",
      "            beam_scores = beam_scores.new([x[0] for x in next_batch_beam])\n",
      "            beam_tokens = input_ids.new([x[1] for x in next_batch_beam])\n",
      "            beam_idx = input_ids.new([x[2] for x in next_batch_beam])\n",
      "\n",
      "            # re-order batch\n",
      "            input_ids = input_ids[beam_idx, :]\n",
      "            input_ids = torch.cat([input_ids, beam_tokens.unsqueeze(1)], dim=-1)\n",
      "\n",
      "            # re-order internal states\n",
      "            if past is not None:\n",
      "                past = self._reorder_cache(past, beam_idx)\n",
      "\n",
      "            # extend attention_mask for new generated input if only decoder\n",
      "            if self.config.is_encoder_decoder is False:\n",
      "                attention_mask = torch.cat(\n",
      "                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n",
      "                )\n",
      "\n",
      "            # update current length\n",
      "            cur_len = cur_len + 1\n",
      "\n",
      "        # finalize all open beam hypotheses and end to generated hypotheses\n",
      "        for batch_idx in range(batch_size):\n",
      "            if done[batch_idx]:\n",
      "                continue\n",
      "\n",
      "            # test that beam scores match previously calculated scores if not eos and batch_idx not done\n",
      "            if eos_token_id is not None and all(\n",
      "                (token_id % vocab_size).item() is not eos_token_id for token_id in next_tokens[batch_idx]\n",
      "            ):\n",
      "                assert torch.all(\n",
      "                    next_scores[batch_idx, :num_beams] == beam_scores.view(batch_size, num_beams)[batch_idx]\n",
      "                ), \"If batch_idx is not done, final next scores: {} have to equal to accumulated beam_scores: {}\".format(\n",
      "                    next_scores[:, :num_beams][batch_idx], beam_scores.view(batch_size, num_beams)[batch_idx],\n",
      "                )\n",
      "\n",
      "            # need to add best num_beams hypotheses to generated hyps\n",
      "            for beam_id in range(num_beams):\n",
      "                effective_beam_id = batch_idx * num_beams + beam_id\n",
      "                final_score = beam_scores[effective_beam_id].item()\n",
      "                final_tokens = input_ids[effective_beam_id]\n",
      "                generated_hyps[batch_idx].add(final_tokens, final_score)\n",
      "\n",
      "        # depending on whether greedy generation is wanted or not define different output_batch_size and output_num_return_sequences_per_batch\n",
      "        output_batch_size = batch_size if do_sample else batch_size * num_return_sequences\n",
      "        output_num_return_sequences_per_batch = 1 if do_sample else num_return_sequences\n",
      "\n",
      "        # select the best hypotheses\n",
      "        sent_lengths = input_ids.new(output_batch_size)\n",
      "        best = []\n",
      "\n",
      "        # retrieve best hypotheses\n",
      "        for i, hypotheses in enumerate(generated_hyps):\n",
      "            sorted_hyps = sorted(hypotheses.beams, key=lambda x: x[0])\n",
      "            for j in range(output_num_return_sequences_per_batch):\n",
      "                effective_batch_idx = output_num_return_sequences_per_batch * i + j\n",
      "                best_hyp = sorted_hyps.pop()[1]\n",
      "                sent_lengths[effective_batch_idx] = len(best_hyp)\n",
      "                best.append(best_hyp)\n",
      "\n",
      "        # shorter batches are filled with pad_token\n",
      "        if sent_lengths.min().item() != sent_lengths.max().item():\n",
      "            assert pad_token_id is not None, \"`Pad_token_id` has to be defined\"\n",
      "            sent_max_len = min(sent_lengths.max().item() + 1, max_length)\n",
      "            decoded = input_ids.new(output_batch_size, sent_max_len).fill_(pad_token_id)\n",
      "\n",
      "            # fill with hypothesis and eos_token_id if necessary\n",
      "            for i, hypo in enumerate(best):\n",
      "                decoded[i, : sent_lengths[i]] = hypo\n",
      "                if sent_lengths[i] < max_length:\n",
      "                    decoded[i, sent_lengths[i]] = eos_token_id\n",
      "        else:\n",
      "            # none of the hypotheses have an eos_token\n",
      "            assert (len(hypo) == max_length for hypo in best)\n",
      "            decoded = torch.stack(best).type(torch.long).to(next(self.parameters()).device)\n",
      "\n",
      "        return decoded\n",
      "\n",
      "    # force one of token_ids to be generated by setting prob of all other tokens to 0.\n",
      "    def _force_token_ids_generation(self, scores, token_ids):\n",
      "        if isinstance(token_ids, int):\n",
      "            token_ids = [token_ids]\n",
      "        all_but_token_ids_mask = torch.tensor(\n",
      "            [x for x in range(self.config.vocab_size) if x not in token_ids],\n",
      "            dtype=torch.long,\n",
      "            device=next(self.parameters()).device,\n",
      "        )\n",
      "        assert len(scores.shape) == 2, \"scores should be of rank 2 with shape: [batch_size, vocab_size]\"\n",
      "        scores[:, all_but_token_ids_mask] = -float(\"inf\")\n",
      "\n",
      "    @staticmethod\n",
      "    def _reorder_cache(past, beam_idx):\n",
      "        reordered_past = []\n",
      "        for layer_past in past:\n",
      "            # get the correct batch idx from layer past batch dim\n",
      "            # batch dim of `past` and `mems` is at 2nd position\n",
      "            reordered_layer_past = [layer_past[:, i].unsqueeze(1).clone().detach() for i in beam_idx]\n",
      "            reordered_layer_past = torch.cat(reordered_layer_past, dim=1)\n",
      "            # check that shape matches\n",
      "            assert reordered_layer_past.shape == layer_past.shape\n",
      "            reordered_past.append(reordered_layer_past)\n",
      "        past = tuple(reordered_past)\n",
      "        return past\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(PreTrainedModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class BertPreTrainedModel(PreTrainedModel):\n",
      "    \"\"\" An abstract class to handle weights initialization and\n",
      "        a simple interface for downloading and loading pretrained models.\n",
      "    \"\"\"\n",
      "\n",
      "    config_class = BertConfig\n",
      "    pretrained_model_archive_map = BERT_PRETRAINED_MODEL_ARCHIVE_MAP\n",
      "    load_tf_weights = load_tf_weights_in_bert\n",
      "    base_model_prefix = \"bert\"\n",
      "\n",
      "    def _init_weights(self, module):\n",
      "        \"\"\" Initialize the weights \"\"\"\n",
      "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
      "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
      "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
      "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
      "        elif isinstance(module, BertLayerNorm):\n",
      "            module.bias.data.zero_()\n",
      "            module.weight.data.fill_(1.0)\n",
      "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
      "            module.bias.data.zero_()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(BertPreTrainedModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "\n",
    "    The model can behave as an encoder (with only self-attention) as well\n",
    "    as a decoder, in which case a layer of cross-attention is added between\n",
    "    the self-attention layers, following the architecture described in `Attention is all you need`_ by Ashish Vaswani,\n",
    "    Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "\n",
    "    To behave as an decoder the model needs to be initialized with the\n",
    "    :obj:`is_decoder` argument of the configuration set to :obj:`True`; an\n",
    "    :obj:`encoder_hidden_states` is expected as an input to the forward pass.\n",
    "\n",
    "    .. _`Attention is all you need`:\n",
    "        https://arxiv.org/abs/1706.03762\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\" Prunes heads of the model.\n",
    "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
    "            See base class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "    Return:\n",
    "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n",
    "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "        pooler_output (:obj:`torch.FloatTensor`: of shape :obj:`(batch_size, hidden_size)`):\n",
    "            Last layer hidden-state of the first token of the sequence (classification token)\n",
    "            further processed by a Linear layer and a Tanh activation function. The Linear\n",
    "            layer weights are trained from the next sentence prediction (classification)\n",
    "            objective during pre-training.\n",
    "\n",
    "            This output is usually *not* a good summary\n",
    "            of the semantic content of the input, you're often better with averaging or pooling\n",
    "            the sequence of hidden-states for the whole input sequence.\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
    "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        from transformers import BertModel, BertTokenizer\n",
    "        import torch\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(input_shape, device=device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        if attention_mask.dim() == 3:\n",
    "            extended_attention_mask = attention_mask[:, None, :, :]\n",
    "        elif attention_mask.dim() == 2:\n",
    "            # Provided a padding mask of dimensions [batch_size, seq_length]\n",
    "            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
    "            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "            if self.config.is_decoder:\n",
    "                batch_size, seq_length = input_shape\n",
    "                seq_ids = torch.arange(seq_length, device=device)\n",
    "                causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n",
    "                causal_mask = causal_mask.to(\n",
    "                    attention_mask.dtype\n",
    "                )  # causal and attention masks must have same type with pytorch version < 1.3\n",
    "                extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
    "            else:\n",
    "                extended_attention_mask = attention_mask[:, None, None, :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n",
    "                    input_shape, attention_mask.shape\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        # If a 2D ou 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "\n",
    "            if encoder_attention_mask.dim() == 3:\n",
    "                encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n",
    "            elif encoder_attention_mask.dim() == 2:\n",
    "                encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Wrong shape for encoder_hidden_shape (shape {}) or encoder_attention_mask (shape {})\".format(\n",
    "                        encoder_hidden_shape, encoder_attention_mask.shape\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            encoder_extended_attention_mask = encoder_extended_attention_mask.to(\n",
    "                dtype=next(self.parameters()).dtype\n",
    "            )  # fp16 compatibility\n",
    "            encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -10000.0\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        if head_mask is not None:\n",
    "            if head_mask.dim() == 1:\n",
    "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\n",
    "            elif head_mask.dim() == 2:\n",
    "                head_mask = (\n",
    "                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n",
    "                )  # We can specify head_mask for each layer\n",
    "            head_mask = head_mask.to(\n",
    "                dtype=next(self.parameters()).dtype\n",
    "            )  # switch to fload if need + fp16 compatibility\n",
    "        else:\n",
    "            head_mask = [None] * self.config.num_hidden_layers\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n",
    "            1:\n",
    "        ]  # add hidden_states and attentions if they are here\n",
    "        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class BertModel(BertPreTrainedModel):\n",
      "    \"\"\"\n",
      "\n",
      "    The model can behave as an encoder (with only self-attention) as well\n",
      "    as a decoder, in which case a layer of cross-attention is added between\n",
      "    the self-attention layers, following the architecture described in `Attention is all you need`_ by Ashish Vaswani,\n",
      "    Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
      "\n",
      "    To behave as an decoder the model needs to be initialized with the\n",
      "    :obj:`is_decoder` argument of the configuration set to :obj:`True`; an\n",
      "    :obj:`encoder_hidden_states` is expected as an input to the forward pass.\n",
      "\n",
      "    .. _`Attention is all you need`:\n",
      "        https://arxiv.org/abs/1706.03762\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, config):\n",
      "        super().__init__(config)\n",
      "        self.config = config\n",
      "\n",
      "        self.embeddings = BertEmbeddings(config)\n",
      "        self.encoder = BertEncoder(config)\n",
      "        self.pooler = BertPooler(config)\n",
      "\n",
      "        self.init_weights()\n",
      "\n",
      "    def get_input_embeddings(self):\n",
      "        return self.embeddings.word_embeddings\n",
      "\n",
      "    def set_input_embeddings(self, value):\n",
      "        self.embeddings.word_embeddings = value\n",
      "\n",
      "    def _prune_heads(self, heads_to_prune):\n",
      "        \"\"\" Prunes heads of the model.\n",
      "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
      "            See base class PreTrainedModel\n",
      "        \"\"\"\n",
      "        for layer, heads in heads_to_prune.items():\n",
      "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
      "\n",
      "    @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids=None,\n",
      "        attention_mask=None,\n",
      "        token_type_ids=None,\n",
      "        position_ids=None,\n",
      "        head_mask=None,\n",
      "        inputs_embeds=None,\n",
      "        encoder_hidden_states=None,\n",
      "        encoder_attention_mask=None,\n",
      "    ):\n",
      "        r\"\"\"\n",
      "    Return:\n",
      "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n",
      "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
      "            Sequence of hidden-states at the output of the last layer of the model.\n",
      "        pooler_output (:obj:`torch.FloatTensor`: of shape :obj:`(batch_size, hidden_size)`):\n",
      "            Last layer hidden-state of the first token of the sequence (classification token)\n",
      "            further processed by a Linear layer and a Tanh activation function. The Linear\n",
      "            layer weights are trained from the next sentence prediction (classification)\n",
      "            objective during pre-training.\n",
      "\n",
      "            This output is usually *not* a good summary\n",
      "            of the semantic content of the input, you're often better with averaging or pooling\n",
      "            the sequence of hidden-states for the whole input sequence.\n",
      "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
      "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
      "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
      "\n",
      "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
      "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
      "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
      "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
      "\n",
      "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      "            heads.\n",
      "\n",
      "    Examples::\n",
      "\n",
      "        from transformers import BertModel, BertTokenizer\n",
      "        import torch\n",
      "\n",
      "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "        model = BertModel.from_pretrained('bert-base-uncased')\n",
      "\n",
      "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
      "        outputs = model(input_ids)\n",
      "\n",
      "        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            input_shape = input_ids.size()\n",
      "        elif inputs_embeds is not None:\n",
      "            input_shape = inputs_embeds.size()[:-1]\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
      "\n",
      "        if attention_mask is None:\n",
      "            attention_mask = torch.ones(input_shape, device=device)\n",
      "        if token_type_ids is None:\n",
      "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
      "\n",
      "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
      "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
      "        if attention_mask.dim() == 3:\n",
      "            extended_attention_mask = attention_mask[:, None, :, :]\n",
      "        elif attention_mask.dim() == 2:\n",
      "            # Provided a padding mask of dimensions [batch_size, seq_length]\n",
      "            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
      "            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
      "            if self.config.is_decoder:\n",
      "                batch_size, seq_length = input_shape\n",
      "                seq_ids = torch.arange(seq_length, device=device)\n",
      "                causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n",
      "                causal_mask = causal_mask.to(\n",
      "                    attention_mask.dtype\n",
      "                )  # causal and attention masks must have same type with pytorch version < 1.3\n",
      "                extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
      "            else:\n",
      "                extended_attention_mask = attention_mask[:, None, None, :]\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n",
      "                    input_shape, attention_mask.shape\n",
      "                )\n",
      "            )\n",
      "\n",
      "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
      "        # masked positions, this operation will create a tensor which is 0.0 for\n",
      "        # positions we want to attend and -10000.0 for masked positions.\n",
      "        # Since we are adding it to the raw scores before the softmax, this is\n",
      "        # effectively the same as removing these entirely.\n",
      "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
      "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
      "\n",
      "        # If a 2D ou 3D attention mask is provided for the cross-attention\n",
      "        # we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]\n",
      "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
      "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
      "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
      "            if encoder_attention_mask is None:\n",
      "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
      "\n",
      "            if encoder_attention_mask.dim() == 3:\n",
      "                encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n",
      "            elif encoder_attention_mask.dim() == 2:\n",
      "                encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n",
      "            else:\n",
      "                raise ValueError(\n",
      "                    \"Wrong shape for encoder_hidden_shape (shape {}) or encoder_attention_mask (shape {})\".format(\n",
      "                        encoder_hidden_shape, encoder_attention_mask.shape\n",
      "                    )\n",
      "                )\n",
      "\n",
      "            encoder_extended_attention_mask = encoder_extended_attention_mask.to(\n",
      "                dtype=next(self.parameters()).dtype\n",
      "            )  # fp16 compatibility\n",
      "            encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -10000.0\n",
      "        else:\n",
      "            encoder_extended_attention_mask = None\n",
      "\n",
      "        # Prepare head mask if needed\n",
      "        # 1.0 in head_mask indicate we keep the head\n",
      "        # attention_probs has shape bsz x n_heads x N x N\n",
      "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
      "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
      "        if head_mask is not None:\n",
      "            if head_mask.dim() == 1:\n",
      "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
      "                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\n",
      "            elif head_mask.dim() == 2:\n",
      "                head_mask = (\n",
      "                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n",
      "                )  # We can specify head_mask for each layer\n",
      "            head_mask = head_mask.to(\n",
      "                dtype=next(self.parameters()).dtype\n",
      "            )  # switch to fload if need + fp16 compatibility\n",
      "        else:\n",
      "            head_mask = [None] * self.config.num_hidden_layers\n",
      "\n",
      "        embedding_output = self.embeddings(\n",
      "            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n",
      "        )\n",
      "        encoder_outputs = self.encoder(\n",
      "            embedding_output,\n",
      "            attention_mask=extended_attention_mask,\n",
      "            head_mask=head_mask,\n",
      "            encoder_hidden_states=encoder_hidden_states,\n",
      "            encoder_attention_mask=encoder_extended_attention_mask,\n",
      "        )\n",
      "        sequence_output = encoder_outputs[0]\n",
      "        pooled_output = self.pooler(sequence_output)\n",
      "\n",
      "        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n",
      "            1:\n",
      "        ]  # add hidden_states and attentions if they are here\n",
      "        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(BertModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_num_labels\": 2,\n",
       "  \"architectures\": null,\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"do_sample\": false,\n",
       "  \"early_stopping\": false,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"is_decoder\": false,\n",
       "  \"is_encoder_decoder\": false,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"length_penalty\": 1.0,\n",
       "  \"max_length\": 20,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"min_length\": 0,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"no_repeat_ngram_size\": 0,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_beams\": 1,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_return_sequences\": 1,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"pruned_heads\": {},\n",
       "  \"repetition_penalty\": 1.0,\n",
       "  \"temperature\": 1.0,\n",
       "  \"top_k\": 50,\n",
       "  \"top_p\": 1.0,\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_bfloat16\": false,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
