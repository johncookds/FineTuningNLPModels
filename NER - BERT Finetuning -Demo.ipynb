{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial code adapted from https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/\n",
    "# which was outdated\n",
    "# https://github.com/huggingface/transformers#Quick-tour-TF-20-training-and-PyTorch-interoperability\n",
    "# provided a good reference to update although written for a different task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./Data/entity-annotated-corpus/ner_dataset.csv\", encoding= 'latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Sentence #'] = data['Sentence #'].apply(lambda x: int(x.split(': ')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>from</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS    Tag\n",
       "0            1      Thousands  NNS      O\n",
       "1            1             of   IN      O\n",
       "2            1  demonstrators  NNS      O\n",
       "3            1           have  VBP      O\n",
       "4            1        marched  VBN      O\n",
       "5            1        through   IN      O\n",
       "6            1         London  NNP  B-geo\n",
       "7            1             to   TO      O\n",
       "8            1        protest   VB      O\n",
       "9            1            the   DT      O\n",
       "10           1            war   NN      O\n",
       "11           1             in   IN      O\n",
       "12           1           Iraq  NNP  B-geo\n",
       "13           1            and   CC      O\n",
       "14           1         demand   VB      O\n",
       "15           1            the   DT      O\n",
       "16           1     withdrawal   NN      O\n",
       "17           1             of   IN      O\n",
       "18           1        British   JJ  B-gpe\n",
       "19           1         troops  NNS      O\n",
       "20           1           from   IN      O\n",
       "21           1           that   DT      O\n",
       "22           1        country   NN      O\n",
       "23           1              .    .      O"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['Sentence #']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O        887908\n",
       "B-geo     37644\n",
       "B-tim     20333\n",
       "B-org     20143\n",
       "I-per     17251\n",
       "B-per     16990\n",
       "I-org     16784\n",
       "B-gpe     15870\n",
       "I-geo      7414\n",
       "I-tim      6528\n",
       "B-art       402\n",
       "B-eve       308\n",
       "I-art       297\n",
       "I-eve       253\n",
       "B-nat       201\n",
       "I-gpe       198\n",
       "I-nat        51\n",
       "Name: Tag, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thousands',\n",
       " 'of',\n",
       " 'demonstrators',\n",
       " 'have',\n",
       " 'marched',\n",
       " 'through',\n",
       " 'London',\n",
       " 'to',\n",
       " 'protest',\n",
       " 'the',\n",
       " 'war',\n",
       " 'in',\n",
       " 'Iraq',\n",
       " 'and',\n",
       " 'demand',\n",
       " 'the',\n",
       " 'withdrawal',\n",
       " 'of',\n",
       " 'British',\n",
       " 'troops',\n",
       " 'from',\n",
       " 'that',\n",
       " 'country',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [[word[0] for word in sentence] for sentence in getter.sentences]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "labels = [[s[2] for s in sentence] for sentence in getter.sentences]\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_values = list(set(data[\"Tag\"].values))\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-art',\n",
       " 'B-geo',\n",
       " 'B-art',\n",
       " 'O',\n",
       " 'I-nat',\n",
       " 'I-per',\n",
       " 'B-tim',\n",
       " 'B-eve',\n",
       " 'B-nat',\n",
       " 'I-tim',\n",
       " 'I-gpe',\n",
       " 'I-org',\n",
       " 'B-per',\n",
       " 'I-geo',\n",
       " 'B-org',\n",
       " 'B-gpe',\n",
       " 'I-eve',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 75\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have py-torch use local GPU\n",
    "device = torch.device(\"cuda\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(sentences, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts, labels = zip(*tokenized_texts_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs).to(torch.int64)\n",
    "val_inputs = torch.tensor(val_inputs).to(torch.int64)\n",
    "tr_tags = torch.tensor(tr_tags).to(torch.int64)\n",
    "val_tags = torch.tensor(val_tags).to(torch.int64)\n",
    "tr_masks = torch.tensor(tr_masks).to(torch.int64)\n",
    "val_masks = torch.tensor(val_masks).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "\"bert-base-cased\",\n",
    "num_labels=len(tag2idx),\n",
    "output_attentions = False,\n",
    "output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels, avoid = 17): # 17 is the PAD token\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    mask = labels_flat != 17\n",
    "    labels_flat = labels_flat[mask]\n",
    "    pred_flat = pred_flat[mask]\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select training parameters and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert.embeddings.word_embeddings.weight',\n",
       " 'bert.embeddings.position_embeddings.weight',\n",
       " 'bert.embeddings.token_type_embeddings.weight',\n",
       " 'bert.embeddings.LayerNorm.weight',\n",
       " 'bert.embeddings.LayerNorm.bias',\n",
       " 'bert.encoder.layer.0.attention.self.query.weight',\n",
       " 'bert.encoder.layer.0.attention.self.query.bias',\n",
       " 'bert.encoder.layer.0.attention.self.key.weight',\n",
       " 'bert.encoder.layer.0.attention.self.key.bias',\n",
       " 'bert.encoder.layer.0.attention.self.value.weight',\n",
       " 'bert.encoder.layer.0.attention.self.value.bias',\n",
       " 'bert.encoder.layer.0.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.0.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.0.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.0.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.0.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.0.output.dense.weight',\n",
       " 'bert.encoder.layer.0.output.dense.bias',\n",
       " 'bert.encoder.layer.0.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.0.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.1.attention.self.query.weight',\n",
       " 'bert.encoder.layer.1.attention.self.query.bias',\n",
       " 'bert.encoder.layer.1.attention.self.key.weight',\n",
       " 'bert.encoder.layer.1.attention.self.key.bias',\n",
       " 'bert.encoder.layer.1.attention.self.value.weight',\n",
       " 'bert.encoder.layer.1.attention.self.value.bias',\n",
       " 'bert.encoder.layer.1.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.1.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.1.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.1.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.1.output.dense.weight',\n",
       " 'bert.encoder.layer.1.output.dense.bias',\n",
       " 'bert.encoder.layer.1.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.1.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.2.attention.self.query.weight',\n",
       " 'bert.encoder.layer.2.attention.self.query.bias',\n",
       " 'bert.encoder.layer.2.attention.self.key.weight',\n",
       " 'bert.encoder.layer.2.attention.self.key.bias',\n",
       " 'bert.encoder.layer.2.attention.self.value.weight',\n",
       " 'bert.encoder.layer.2.attention.self.value.bias',\n",
       " 'bert.encoder.layer.2.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.2.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.2.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.2.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.2.output.dense.weight',\n",
       " 'bert.encoder.layer.2.output.dense.bias',\n",
       " 'bert.encoder.layer.2.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.2.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.3.attention.self.query.weight',\n",
       " 'bert.encoder.layer.3.attention.self.query.bias',\n",
       " 'bert.encoder.layer.3.attention.self.key.weight',\n",
       " 'bert.encoder.layer.3.attention.self.key.bias',\n",
       " 'bert.encoder.layer.3.attention.self.value.weight',\n",
       " 'bert.encoder.layer.3.attention.self.value.bias',\n",
       " 'bert.encoder.layer.3.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.3.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.3.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.3.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.3.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.3.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.3.output.dense.weight',\n",
       " 'bert.encoder.layer.3.output.dense.bias',\n",
       " 'bert.encoder.layer.3.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.3.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.4.attention.self.query.weight',\n",
       " 'bert.encoder.layer.4.attention.self.query.bias',\n",
       " 'bert.encoder.layer.4.attention.self.key.weight',\n",
       " 'bert.encoder.layer.4.attention.self.key.bias',\n",
       " 'bert.encoder.layer.4.attention.self.value.weight',\n",
       " 'bert.encoder.layer.4.attention.self.value.bias',\n",
       " 'bert.encoder.layer.4.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.4.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.4.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.4.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.4.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.4.output.dense.weight',\n",
       " 'bert.encoder.layer.4.output.dense.bias',\n",
       " 'bert.encoder.layer.4.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.4.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.5.attention.self.query.weight',\n",
       " 'bert.encoder.layer.5.attention.self.query.bias',\n",
       " 'bert.encoder.layer.5.attention.self.key.weight',\n",
       " 'bert.encoder.layer.5.attention.self.key.bias',\n",
       " 'bert.encoder.layer.5.attention.self.value.weight',\n",
       " 'bert.encoder.layer.5.attention.self.value.bias',\n",
       " 'bert.encoder.layer.5.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.5.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.5.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.5.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.5.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.5.output.dense.weight',\n",
       " 'bert.encoder.layer.5.output.dense.bias',\n",
       " 'bert.encoder.layer.5.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.5.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.6.attention.self.query.weight',\n",
       " 'bert.encoder.layer.6.attention.self.query.bias',\n",
       " 'bert.encoder.layer.6.attention.self.key.weight',\n",
       " 'bert.encoder.layer.6.attention.self.key.bias',\n",
       " 'bert.encoder.layer.6.attention.self.value.weight',\n",
       " 'bert.encoder.layer.6.attention.self.value.bias',\n",
       " 'bert.encoder.layer.6.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.6.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.6.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.6.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.6.output.dense.weight',\n",
       " 'bert.encoder.layer.6.output.dense.bias',\n",
       " 'bert.encoder.layer.6.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.6.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.7.attention.self.query.weight',\n",
       " 'bert.encoder.layer.7.attention.self.query.bias',\n",
       " 'bert.encoder.layer.7.attention.self.key.weight',\n",
       " 'bert.encoder.layer.7.attention.self.key.bias',\n",
       " 'bert.encoder.layer.7.attention.self.value.weight',\n",
       " 'bert.encoder.layer.7.attention.self.value.bias',\n",
       " 'bert.encoder.layer.7.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.7.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.7.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.7.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.7.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.7.output.dense.weight',\n",
       " 'bert.encoder.layer.7.output.dense.bias',\n",
       " 'bert.encoder.layer.7.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.7.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.8.attention.self.query.weight',\n",
       " 'bert.encoder.layer.8.attention.self.query.bias',\n",
       " 'bert.encoder.layer.8.attention.self.key.weight',\n",
       " 'bert.encoder.layer.8.attention.self.key.bias',\n",
       " 'bert.encoder.layer.8.attention.self.value.weight',\n",
       " 'bert.encoder.layer.8.attention.self.value.bias',\n",
       " 'bert.encoder.layer.8.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.8.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.8.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.8.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.8.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.8.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.8.output.dense.weight',\n",
       " 'bert.encoder.layer.8.output.dense.bias',\n",
       " 'bert.encoder.layer.8.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.8.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.9.attention.self.query.weight',\n",
       " 'bert.encoder.layer.9.attention.self.query.bias',\n",
       " 'bert.encoder.layer.9.attention.self.key.weight',\n",
       " 'bert.encoder.layer.9.attention.self.key.bias',\n",
       " 'bert.encoder.layer.9.attention.self.value.weight',\n",
       " 'bert.encoder.layer.9.attention.self.value.bias',\n",
       " 'bert.encoder.layer.9.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.9.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.9.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.9.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.9.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.9.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.9.output.dense.weight',\n",
       " 'bert.encoder.layer.9.output.dense.bias',\n",
       " 'bert.encoder.layer.9.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.9.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.10.attention.self.query.weight',\n",
       " 'bert.encoder.layer.10.attention.self.query.bias',\n",
       " 'bert.encoder.layer.10.attention.self.key.weight',\n",
       " 'bert.encoder.layer.10.attention.self.key.bias',\n",
       " 'bert.encoder.layer.10.attention.self.value.weight',\n",
       " 'bert.encoder.layer.10.attention.self.value.bias',\n",
       " 'bert.encoder.layer.10.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.10.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.10.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.10.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.10.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.10.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.10.output.dense.weight',\n",
       " 'bert.encoder.layer.10.output.dense.bias',\n",
       " 'bert.encoder.layer.10.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.10.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.11.attention.self.query.weight',\n",
       " 'bert.encoder.layer.11.attention.self.query.bias',\n",
       " 'bert.encoder.layer.11.attention.self.key.weight',\n",
       " 'bert.encoder.layer.11.attention.self.key.bias',\n",
       " 'bert.encoder.layer.11.attention.self.value.weight',\n",
       " 'bert.encoder.layer.11.attention.self.value.bias',\n",
       " 'bert.encoder.layer.11.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.11.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.11.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.11.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.11.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.11.output.dense.weight',\n",
       " 'bert.encoder.layer.11.output.dense.bias',\n",
       " 'bert.encoder.layer.11.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.11.output.LayerNorm.bias',\n",
       " 'bert.pooler.dense.weight',\n",
       " 'bert.pooler.dense.bias',\n",
       " 'classifier.weight',\n",
       " 'classifier.bias']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can view names of trainable parameters in model\n",
    "[itm[0] for itm in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "lr=3e-5,\n",
    "eps=1e-8\n",
    ")\n",
    "epochs = 3\n",
    "max_grad_norm = 1.0\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a48eb440ab443a5a03b2bb885a37656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe9fde5a15640258fa9755fe7a5b6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-63555e249f4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Perform a backward pass to calculate the gradients.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;31m# track train loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\johnc\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\johnc\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "for _ in tnrange(epochs, desc=\"Epoch\"):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    for step, batch in tqdm_notebook(enumerate(train_dataloader)):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Always clear any previously calculated gradients before performing a backward pass.\n",
    "        model.zero_grad()\n",
    "        # forward pass\n",
    "        # This will return the loss (rather than the model output)\n",
    "        # because we have provided the `labels`.\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "        # get the loss\n",
    "        loss = outputs[0]\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        total_loss += loss.item()\n",
    "        # Clip the norm of the gradient\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    metrics.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    # Reset the validation loss for this epoch.\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients,\n",
    "        # saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have not provided labels.\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "\n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    metrics.append(eval_loss)\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
    "    valid_tags = [tag_values[l_i] for l in true_labels\n",
    "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
    "    print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
